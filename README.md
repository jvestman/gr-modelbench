# ğŸ¦™ gr-modelbench

A **Gradio-based benchmarking and evaluation tool** for generating HTML pages with **remote Ollama models**, running **batch (â€œbatch modeâ€) generations**, previewing results, and performing **human evaluations** with structured scoring.

![Screenshot displaying batcch generation in progress](screenshot.png "Screenshot")

Designed for:
- Comparing multiple LLMs
- Comparing multiple prompt templates
- Manual prompt experimentation
- Human-in-the-loop evaluation
- Reproducible logging and analysis

---

## âœ¨ Features

### ğŸ”Œ Remote Ollama Integration
- Connect to any remote Ollama instance via URL
- Discover available models dynamically
- Execute `generate` requests remotely

### ğŸ§© Batch Mode (Batch Generation)
- Select **multiple models**
- Select **multiple prompt templates**
- Or provide a **manual prompt** (overrides templates)
- Jobs run **sequentially in the background**
- Track status per job (pending / running / done / error)

### ğŸ–¥ï¸ HTML Rendering
- View raw generated HTML
- Render HTML preview safely
- Open preview in a **new browser tab**
- If fenced HTML exists, only fenced content is rendered

### ğŸ§‘â€âš–ï¸ Human Evaluation
- Structured 5-point evaluation scales for:
  - Color & Typography
  - Layout & Structure
  - Correctness
  - Functionality
- Clear criteria embedded directly in UI
- Free-form qualitative feedback
- Evaluations stored per generation run

### ğŸ“Š Experiment Logging
- Each generation gets a unique **UUID**
- Metrics logged in **EAV (entityâ€“attributeâ€“value)** format
- Logged attributes include:
  - Model name
  - Prompt template name
  - SHA256 hash of full prompt
  - Manual vs template prompt source
  - Tokens/sec
  - Eval count
  - Prompt count
  - Generation time
  - Random seed (when available)
- Append-only CSV logs for easy analysis

### ğŸ§± Modular Architecture
Code is split into focused modules:
- UI components
- Patch mode execution
- Rendering
- Prompt handling
- Evaluation logic
- Persistence & logging

This keeps the codebase **LLM-friendly**, maintainable, and extensible.

---

## ğŸ“ Project Structure

```text
.
â”œâ”€â”€ main.py                 # App entry point
â”œâ”€â”€ ui_components.py        # Gradio UI layout
â”œâ”€â”€ patch_mode.py           # Batch job creation & execution
â”œâ”€â”€ ollama.py               # Ollama API integration
â”œâ”€â”€ prompts.py              # Prompt templates & builder
â”œâ”€â”€ rendering.py            # HTML preview handling
â”œâ”€â”€ evaluation.py           # Save human evaluations
â”œâ”€â”€ evaluation_scales.py    # Scoring scales & criteria
â”œâ”€â”€ persistence.py          # Persist Ollama URL, settings
â”œâ”€â”€ utils.py                # Helpers (HTML fence extraction, hashing, etc.)
â”œâ”€â”€ previews/               # Generated HTML previews
â”œâ”€â”€ log.csv                 # Generation metrics (EAV format)
â”œâ”€â”€ prompt.yml              # Prompt template definitions
â””â”€â”€ README.md
